{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スマートベータポートフォリオとポートフォリオの最適化\n",
    "\n",
    "## 概要\n",
    "\n",
    "スマートベータには広い意味がありますが、実際には、インデックスから株式のユニバースを使用し、時価総額加重以外の加重スキームを適用すると、一種のスマートベータファンドと見なすことができます。 スマートベータポートフォリオは、一般に、投資家に特定の市場への多様な幅広いエクスポージャーを提供しながら、価格を予測すると考えられる1つ以上のタイプの市場特性（またはファクター）へのエクスポージャーまたは「ベータ」を提供します。 スマートベータポートフォリオは通常、モメンタム、収益の質、低ボラティリティ、配当、またはいくつかの組み合わせを対象としています。 スマートベータポートフォリオは、通常、リバランスが頻繁に行われることはなく、受動的に管理される比較的単純なルールまたはアルゴリズムに従います。 これらのタイプのファンドへのモデル変更もまれであり、米国に焦点を当てたミューチュアルファンドまたはETFの場合、米国証券取引委員会への目論見書の提出が必要です。\n",
    "\n",
    "対照的に、純粋にアルファに焦点を合わせた定量的ファンドは、ポートフォリオを作成するために複数のモデルまたはアルゴリズムを使用する場合があります。 ポートフォリオマネージャーは、モデルのタイプをアップグレードまたは変更する際の裁量と、株式ベンチマークと比較してパフォーマンスを最大化するためにポートフォリオをリバランスする頻度を保持します。 マネージャーは、ポートフォリオの株を不足させる裁量権を持っている場合があります。\n",
    "\n",
    "ポートフォリオマネージャーとして、いくつかの異なるポートフォリオの重み付け方法を試してみたいとしてます。 \n",
    "\n",
    "ポートフォリオを設計する1つの方法は、過去の傾向に基づいて、より良い結果を生み出す株式を示す特定の会計基準（ファンダメンタル）を調べることです。 \n",
    "\n",
    "たとえば、配当を発行する株式は、そうでない株式よりもパフォーマンスが高い傾向があるという仮説から始めます。これは必ずしもすべての企業に当てはまるとは限りません。たとえば、Appleは配当金を発行していませんが、過去の実績は良好です。配当を支払う株式に関する仮説は、次のようになります。\n",
    "\n",
    "定期的に配当を行う企業は、利用可能な現金をより慎重に配分することもでき、株主の利益を優先することをより意識していることを示している可能性があります。たとえば、CEOは、収益の低いペットプロジェクトに現金を再投資することを決定する場合があります。または、CEOが分析を行い、企業内での再投資は分散ポートフォリオと比較して収益が低いことを確認し、株主が現金（配当の形で）を与えられた方がより良いサービスを受けると判断する場合があります。したがって、この仮説によれば、配当は、会社の業績（収益とキャッシュフローの観点から）の代用であるだけでなく、会社が株主の最善の利益のために行動していることの合図でもあります。もちろん、これが実際に機能するかどうかをテストすることは重要です。 \n",
    "\n",
    "また、ETFにすることができるポートフォリオを設計したいという別の仮説があるかもしれません。 投資家はパッシブベータファンドに投資したいと思うかもしれませんが、投資のリスクエクスポージャーを減らしたい（ボラティリティを減らしたい）と思うかもしれません。 インデックスと同様のリターンを生み出す低ボラティリティファンドを持つという目標は、投資期間が短い投資家にとって魅力的である可能性があり、リスク回避性が高くなります。 \n",
    "\n",
    "以上をふまえて、ポートフォリオの目的を、ポートフォリオの分散を最小限に抑えながら、インデックスを厳密に追跡するポートフォリオを設計すること、とします。 また、このポートフォリオがボラティリティの低いインデックスのリターンと一致する場合、リスク調整後リターンは高くなります（同じリターン、ボラティリティが低くなります）。\n",
    "\n",
    "スマートベータETFは、これら2つの一般的な方法（とりわけ）の両方を使用して設計できます。代替の重み付けと最小ボラティリティETFです。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import plotly.offline as offline_py\n",
    "offline_py.init_notebook_mode(connected=True)\n",
    "\n",
    "color_scheme = {\n",
    "    'index': '#B6B2CF',\n",
    "    'etf': '#2D3ECF',\n",
    "    'tracking_error': '#6F91DE',\n",
    "    'df_header': 'silver',\n",
    "    'df_value': 'white',\n",
    "    'df_line': 'silver',\n",
    "    'heatmap_colorscale': [(0, '#6F91DE'), (0.5, 'grey'), (1, 'red')],\n",
    "    'background_label': '#9dbdd5',\n",
    "    'low_value': '#B6B2CF',\n",
    "    'high_value': '#2D3ECF',\n",
    "    'y_axis_2_text_color': 'grey',\n",
    "    'shadow': 'rgba(0, 0, 0, 0.75)',\n",
    "    'major_line': '#2D3ECF',\n",
    "    'minor_line': '#B6B2CF',\n",
    "    'main_line': 'black'}\n",
    "\n",
    "\n",
    "def generate_config():\n",
    "    return {'showLink': False, 'displayModeBar': False, 'showAxisRangeEntryBoxes': True}\n",
    "\n",
    "\n",
    "def _generate_hover_text(x_text, y_text, z_values, x_label, y_label, z_label):\n",
    "    float_to_str = np.vectorize('{:.7f}'.format)\n",
    "\n",
    "    x_hover_text_values = np.tile(x_text.strftime(\"%Y-%m-%d\") , (len(y_text), 1))\n",
    "    y_hover_text_values = np.tile(y_text, (len(x_text), 1))\n",
    "\n",
    "    padding_len = np.full(3, max(len(x_label), len(y_label), len(z_label))) - [len(x_label), len(y_label), len(z_label)]\n",
    "\n",
    "    # Additional padding added to ticker and date to align\n",
    "    hover_text = x_label + ':  ' + padding_len[0] * ' ' + x_hover_text_values + '<br>' + y_label + ':  ' + padding_len[1] * ' ' + y_hover_text_values.T + '<br>' +  z_label + ': ' + padding_len[2] * ' ' + float_to_str(z_values)\n",
    "\n",
    "    return hover_text\n",
    "\n",
    "\n",
    "def _generate_heatmap_trace(df, x_label, y_label, z_label, scale_min, scale_max):\n",
    "    hover_text = _generate_hover_text(df.index, df.columns, df.values.T, x_label, y_label, z_label)\n",
    "\n",
    "    return go.Heatmap(\n",
    "        x=df.index,\n",
    "        y=df.columns,\n",
    "        z=df.values.T,\n",
    "        zauto=False,\n",
    "        zmax=scale_max,\n",
    "        zmin=scale_min,\n",
    "        colorscale=color_scheme['heatmap_colorscale'],\n",
    "        text=hover_text,\n",
    "        hoverinfo='text')\n",
    "\n",
    "\n",
    "\n",
    "def _sanatize_string(string):\n",
    "    return ''.join([i for i in string if i.isalpha()])\n",
    "\n",
    "def plot_weights(weights, title):\n",
    "    config = generate_config()\n",
    "    graph_path = 'graphs/{}.html'.format(_sanatize_string(title))\n",
    "    trace = _generate_heatmap_trace(weights.sort_index(axis=1, ascending=False), 'Date', 'Ticker', 'Weight', 0.0, 0.2)\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        xaxis={'title': 'Dates'},\n",
    "        yaxis={'title': 'Tickers'})\n",
    "\n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    offline_py.plot(fig, config=config, filename=graph_path, auto_open=False)\n",
    "    display(HTML('The graph for {} is too large. You can view it <a href=\"{}\" target=\"_blank\">here</a>.'\n",
    "                 .format(title, graph_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 市場データ\n",
    "### データロード\n",
    "株価のユニバースとして、流動性が高いと考えられる、取引金額ボリュームの多いものを採用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/project_3/eod-quotemedia.csv')\n",
    "\n",
    "percent_top_dollar = 0.2\n",
    "high_volume_symbols = project_helper.large_dollar_volume_stocks(df, 'adj_close', 'adj_volume', percent_top_dollar)\n",
    "df = df[df['ticker'].isin(high_volume_symbols)]\n",
    "\n",
    "close = df.reset_index().pivot(index='date', columns='ticker', values='adj_close')\n",
    "volume = df.reset_index().pivot(index='date', columns='ticker', values='adj_volume')\n",
    "dividends = df.reset_index().pivot(index='date', columns='ticker', values='dividends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nikkei255 = pd.read_csv('../nikkei255.csv', index_col=0)\n",
    "path = os.path.join(os.environ['CSVDIR'] ,'daily')\n",
    "print(path)\n",
    "\n",
    "li_close = []\n",
    "li_volume = []\n",
    "li_dividend = []\n",
    "\n",
    "for code, company in tqdm(nikkei255.iterrows()):\n",
    "    # load CSV files\n",
    "    df_each_close = pd.read_csv(os.path.join(path, str(code) + '.csv'), usecols=['date', 'close'])\n",
    "    df_each_volume  = pd.read_csv(os.path.join(path, str(code) + '.csv'), usecols=['date', 'volume'])\n",
    "    df_each_dividend   = pd.read_csv(os.path.join(path, str(code) + '.csv'), usecols=['date', 'dividend'])\n",
    "\n",
    "    # Set date time index\n",
    "    df_each_close['date'] = pd.to_datetime(df_each_close['date'])\n",
    "    df_each_close = df_each_close.set_index('date')\n",
    "    df_each_volume['date'] = pd.to_datetime(df_each_volume['date'])\n",
    "    df_each_volume = df_each_volume.set_index('date')\n",
    "    df_each_dividend['date'] = pd.to_datetime(df_each_dividend['date'])\n",
    "    df_each_dividend = df_each_dividend.set_index('date')\n",
    "\n",
    "    # Set compnay name to each columns\n",
    "    df_each_close = df_each_close.rename(columns={'close': company['name']})\n",
    "    df_each_volume = df_each_volume.rename(columns={'volume': company['name']})\n",
    "    df_each_dividend = df_each_dividend.rename(columns={'dividend': company['name']})\n",
    "\n",
    "    li_close.append(df_each_close)\n",
    "    li_volume.append(df_each_volume)\n",
    "    li_dividend.append(df_each_dividend)\n",
    "\n",
    "close = pd.concat(li_close, axis = 1)\n",
    "volume = pd.concat(li_volume, axis = 1)\n",
    "dividend = pd.concat(li_dividend, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "To see what one of these 2-d matrices looks like, let's take a look at the closing prices matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: スマート・ベータ・ポートフォリオ\n",
    "このパート1では、配当利回り(dividend yield)を使用してポートフォリオを構築し、ポートフォリオのウェイトを選択します。このようなポートフォリオは、スマートベータETFに組み込むことができます。 このポートフォリオを時価総額加重指数と比較して、パフォーマンスを確認します。\n",
    "\n",
    "実際には、おそらくデータベンダーからインデックスの重みを取得しますが、ここでは、時価総額加重インデックスをシミュレートします。\n",
    "\n",
    "##インデックスの重み\n",
    "使用するインデックスは、大量の株式に基づいています。 `generate_yen_volume_weights`を実装して、このインデックスの重みを生成します。 日付ごとに、その日付で取引された金額(円)のボリュームに基づいてウェイトを生成します。 たとえば、以下が終値と出来高データであると仮定します。 \n",
    "```\n",
    "                 Prices\n",
    "               A         B         ...\n",
    "2013-07-08     2         2         ...\n",
    "2013-07-09     5         6         ...\n",
    "2013-07-10     1         2         ...\n",
    "2013-07-11     6         5         ...\n",
    "...            ...       ...       ...\n",
    "\n",
    "                 Volume\n",
    "               A         B         ...\n",
    "2013-07-08     100       340       ...\n",
    "2013-07-09     240       220       ...\n",
    "2013-07-10     120       500       ...\n",
    "2013-07-11     10        100       ...\n",
    "...            ...       ...       ...\n",
    "```\n",
    "関数 `generate_yen_volume_weights`から作成された重みは次のようになります。 \n",
    "```\n",
    "               A         B         ...\n",
    "2013-07-08     0.126..   0.194..   ...\n",
    "2013-07-09     0.759..   0.377..   ...\n",
    "2013-07-10     0.075..   0.285..   ...\n",
    "2013-07-11     0.037..   0.142..   ...\n",
    "...            ...       ...       ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_yen_volume_weights(close, volume):\n",
    "    \"\"\"\n",
    "    Generate dollar volume weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close price for each ticker and date\n",
    "    volume : str\n",
    "        Volume for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    yen_volume_weights : DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    yen_volume_weights : DataFrame\n",
    "        The yen volume weights for each ticker and date\n",
    "    \"\"\"\n",
    "    assert close.index.equals(volume.index)\n",
    "    assert close.columns.equals(volume.columns)\n",
    "    \n",
    "    yen_volume = close * volume\n",
    "    date_sum_of_yen_volume = yen_volume.sum(axis=1)\n",
    "    yen_volume_weights = yen_volume.div(date_sum_of_dollar_volume,axis=0)\n",
    "\n",
    "    return yen_volume_weights"
   ]
  },
  {
   "source": [
    "### データ可視化\n",
    "`generate_yen_volume_weights`を使用してインデックスの重みを生成し、ヒートマップで表示します。 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_weights = generate_yen_volume_weights(close, volume)\n",
    "plot_weights(index_weights, 'Index Weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Weights\n",
    "Now that we have the index weights, let's choose the portfolio weights based on dividend. You would normally calculate the weights based on trailing dividend yield, but we'll simplify this by just calculating the total dividend yield over time.\n",
    "\n",
    "Implement `calculate_dividend_weights` to return the weights for each stock based on its total dividend yield over time. This is similar to generating the weight for the index, but it's using dividend data instead.\n",
    "For example, assume the following is `dividends` data:\n",
    "```\n",
    "                 Prices\n",
    "               A         B\n",
    "2013-07-08     0         0\n",
    "2013-07-09     0         1\n",
    "2013-07-10     0.5       0\n",
    "2013-07-11     0         0\n",
    "2013-07-12     2         0\n",
    "...            ...       ...\n",
    "```\n",
    "The weights created from the function `calculate_dividend_weights` should be the following:\n",
    "```\n",
    "               A         B\n",
    "2013-07-08     NaN       NaN\n",
    "2013-07-09     0         1\n",
    "2013-07-10     0.333..   0.666..\n",
    "2013-07-11     0.333..   0.666..\n",
    "2013-07-12     0.714..   0.285..\n",
    "...            ...       ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dividend_weights(dividends):\n",
    "    \"\"\"\n",
    "    Calculate dividend weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dividends : DataFrame\n",
    "        Dividend for each stock and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dividend_weights : DataFrame\n",
    "        Weights for each stock and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    dividends_cumsum = dividends.cumsum()\n",
    "    date_sum = dividends_cumsum.sum(axis=1)\n",
    "    dividend_weights = dividends_cumsum.div(date_sum, axis=0)\n",
    "\n",
    "    return dividend_weights\n",
    "\n",
    "project_tests.test_calculate_dividend_weights(calculate_dividend_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Just like the index weights, let's generate the ETF weights and view them using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_weights = calculate_dividend_weights(dividends)\n",
    "project_helper.plot_weights(etf_weights, 'ETF Weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returns\n",
    "Implement `generate_returns` to generate returns data for all the stocks and dates from price data. You might notice we're implementing returns and not log returns. Since we're not dealing with volatility, we don't have to use log returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_returns(prices):\n",
    "    \"\"\"\n",
    "    Generate returns for ticker and date.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prices : DataFrame\n",
    "        Price for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    returns : Dataframe\n",
    "        The returns for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "\n",
    "    return (prices/prices.shift(1))-1\n",
    "\n",
    "project_tests.test_generate_returns(generate_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the closing returns using `generate_returns` and view them using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = generate_returns(close)\n",
    "project_helper.plot_returns(returns, 'Close Returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Returns\n",
    "With the returns of each stock computed, we can use it to compute the returns for an index or ETF. Implement `generate_weighted_returns` to create weighted returns using the returns and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weighted_returns(returns, weights):\n",
    "    \"\"\"\n",
    "    Generate weighted returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    weights : DataFrame\n",
    "        Weights for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weighted_returns : DataFrame\n",
    "        Weighted returns for each ticker and date\n",
    "    \"\"\"\n",
    "    assert returns.index.equals(weights.index)\n",
    "    assert returns.columns.equals(weights.columns)\n",
    "    \n",
    "    #TODO: Implement function\n",
    "\n",
    "    return returns * weights\n",
    "\n",
    "project_tests.test_generate_weighted_returns(generate_weighted_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the ETF and index returns using `generate_weighted_returns` and view them using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_weighted_returns = generate_weighted_returns(returns, index_weights)\n",
    "etf_weighted_returns = generate_weighted_returns(returns, etf_weights)\n",
    "project_helper.plot_returns(index_weighted_returns, 'Index Returns')\n",
    "project_helper.plot_returns(etf_weighted_returns, 'ETF Returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Returns\n",
    "To compare performance between the ETF and Index, we're going to calculate the tracking error. Before we do that, we first need to calculate the index and ETF comulative returns. Implement `calculate_cumulative_returns` to calculate the cumulative returns over time given the returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cumulative_returns(returns):\n",
    "    \"\"\"\n",
    "    Calculate cumulative returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cumulative_returns : Pandas Series\n",
    "        Cumulative returns for each date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    #print(returns.sum(axis=1)+1)\n",
    "    return (returns.sum(axis=1)+1).cumprod()\n",
    "\n",
    "project_tests.test_calculate_cumulative_returns(calculate_cumulative_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the ETF and index cumulative returns using `calculate_cumulative_returns` and compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_weighted_cumulative_returns = calculate_cumulative_returns(index_weighted_returns)\n",
    "etf_weighted_cumulative_returns = calculate_cumulative_returns(etf_weighted_returns)\n",
    "project_helper.plot_benchmark_returns(index_weighted_cumulative_returns, etf_weighted_cumulative_returns, 'Smart Beta ETF vs Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Error\n",
    "In order to check the performance of the smart beta portfolio, we can calculate the annualized tracking error against the index. Implement `tracking_error` to return the tracking error between the ETF and benchmark.\n",
    "\n",
    "For reference, we'll be using the following annualized tracking error function:\n",
    "$$ TE = \\sqrt{252} * SampleStdev(r_p - r_b) $$\n",
    "\n",
    "Where $ r_p $ is the portfolio/ETF returns and $ r_b $ is the benchmark returns.\n",
    "\n",
    "_Note: When calculating the sample standard deviation, the delta degrees of freedom is 1, which is the also the default value._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracking_error(benchmark_returns_by_date, etf_returns_by_date):\n",
    "    \"\"\"\n",
    "    Calculate the tracking error.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    benchmark_returns_by_date : Pandas Series\n",
    "        The benchmark returns for each date\n",
    "    etf_returns_by_date : Pandas Series\n",
    "        The ETF returns for each date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tracking_error : float\n",
    "        The tracking error\n",
    "    \"\"\"\n",
    "    assert benchmark_returns_by_date.index.equals(etf_returns_by_date.index)\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    error = etf_returns_by_date - benchmark_returns_by_date\n",
    "    #print(error.std())\n",
    "    return np.sqrt(252) * error.std()\n",
    "\n",
    "project_tests.test_tracking_error(tracking_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's generate the tracking error using `tracking_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_beta_tracking_error = tracking_error(np.sum(index_weighted_returns, 1), np.sum(etf_weighted_returns, 1))\n",
    "print('Smart Beta Tracking Error: {}'.format(smart_beta_tracking_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Portfolio Optimization\n",
    "\n",
    "Now, let's create a second portfolio.  We'll still reuse the market cap weighted index, but this will be independent of the dividend-weighted portfolio that we created in part 1.\n",
    "\n",
    "We want to both minimize the portfolio variance and also want to closely track a market cap weighted index.  In other words, we're trying to minimize the distance between the weights of our portfolio and the weights of the index.\n",
    "\n",
    "$Minimize \\left [ \\sigma^2_p + \\lambda \\sqrt{\\sum_{1}^{m}(weight_i - indexWeight_i)^2} \\right  ]$ where $m$ is the number of stocks in the portfolio, and $\\lambda$ is a scaling factor that you can choose.\n",
    "\n",
    "Why are we doing this? One way that investors evaluate a fund is by how well it tracks its index. The fund is still expected to deviate from the index within a certain range in order to improve fund performance.  A way for a fund to track the performance of its benchmark is by keeping its asset weights similar to the weights of the index.  We’d expect that if the fund has the same stocks as the benchmark, and also the same weights for each stock as the benchmark, the fund would yield about the same returns as the benchmark. By minimizing a linear combination of both the portfolio risk and distance between portfolio and benchmark weights, we attempt to balance the desire to minimize portfolio variance with the goal of tracking the index.\n",
    "\n",
    "\n",
    "## Covariance\n",
    "Implement `get_covariance_returns` to calculate the covariance of the `returns`. We'll use this to calculate the portfolio variance.\n",
    "\n",
    "If we have $m$ stock series, the covariance matrix is an $m \\times m$ matrix containing the covariance between each pair of stocks.  We can use [`Numpy.cov`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html) to get the covariance.  We give it a 2D array in which each row is a stock series, and each column is an observation at the same period of time. For any `NaN` values, you can replace them with zeros using the [`DataFrame.fillna`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) function.\n",
    "\n",
    "The covariance matrix $\\mathbf{P} = \n",
    "\\begin{bmatrix}\n",
    "\\sigma^2_{1,1} & ... & \\sigma^2_{1,m} \\\\ \n",
    "... & ... & ...\\\\\n",
    "\\sigma_{m,1} & ... & \\sigma^2_{m,m}  \\\\\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariance_returns(returns):\n",
    "    \"\"\"\n",
    "    Calculate covariance matrices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    returns_covariance  : 2 dimensional Ndarray\n",
    "        The covariance of the returns\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    #print(returns.fillna(0).cov())\n",
    "    \n",
    "    return returns.fillna(0).cov().values\n",
    "\n",
    "project_tests.test_get_covariance_returns(get_covariance_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's look at the covariance generated from `get_covariance_returns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_returns = get_covariance_returns(returns)\n",
    "covariance_returns = pd.DataFrame(covariance_returns, returns.columns, returns.columns)\n",
    "\n",
    "covariance_returns_correlation = np.linalg.inv(np.diag(np.sqrt(np.diag(covariance_returns))))\n",
    "covariance_returns_correlation = pd.DataFrame(\n",
    "    covariance_returns_correlation.dot(covariance_returns).dot(covariance_returns_correlation),\n",
    "    covariance_returns.index,\n",
    "    covariance_returns.columns)\n",
    "\n",
    "project_helper.plot_covariance_returns_correlation(\n",
    "    covariance_returns_correlation,\n",
    "    'Covariance Returns Correlation Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### portfolio variance\n",
    "We can write the portfolio variance $\\sigma^2_p = \\mathbf{x^T} \\mathbf{P} \\mathbf{x}$\n",
    "\n",
    "Recall that the $\\mathbf{x^T} \\mathbf{P} \\mathbf{x}$ is called the quadratic form.\n",
    "We can use the cvxpy function `quad_form(x,P)` to get the quadratic form.\n",
    "\n",
    "### Distance from index weights\n",
    "We want portfolio weights that track the index closely.  So we want to minimize the distance between them.\n",
    "Recall from the Pythagorean theorem that you can get the distance between two points in an x,y plane by adding the square of the x and y distances and taking the square root.  Extending this to any number of dimensions is called the L2 norm.  So: $\\sqrt{\\sum_{1}^{n}(weight_i - indexWeight_i)^2}$  Can also be written as $\\left \\| \\mathbf{x} - \\mathbf{index} \\right \\|_2$.  There's a cvxpy function called [norm()](https://www.cvxpy.org/api_reference/cvxpy.atoms.other_atoms.html#norm)\n",
    "`norm(x, p=2, axis=None)`.  The default is already set to find an L2 norm, so you would pass in one argument, which is the difference between your portfolio weights and the index weights.\n",
    "\n",
    "### objective function\n",
    "We want to minimize both the portfolio variance and the distance of the portfolio weights from the index weights.\n",
    "We also want to choose a `scale` constant, which is $\\lambda$ in the expression. \n",
    "\n",
    "$\\mathbf{x^T} \\mathbf{P} \\mathbf{x} + \\lambda \\left \\| \\mathbf{x} - \\mathbf{index} \\right \\|_2$\n",
    "\n",
    "\n",
    "This lets us choose how much priority we give to minimizing the difference from the index, relative to minimizing the variance of the portfolio.  If you choose a higher value for `scale` ($\\lambda$).\n",
    "\n",
    "We can find the objective function using cvxpy `objective = cvx.Minimize()`.  Can you guess what to pass into this function?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constraints\n",
    "We can also define our constraints in a list.  For example, you'd want the weights to sum to one. So $\\sum_{1}^{n}x = 1$.  You may also need to go long only, which means no shorting, so no negative weights.  So $x_i >0 $ for all $i$. you could save a variable as `[x >= 0, sum(x) == 1]`, where x was created using `cvx.Variable()`.\n",
    "\n",
    "### optimization\n",
    "So now that we have our objective function and constraints, we can solve for the values of $\\mathbf{x}$.\n",
    "cvxpy has the constructor `Problem(objective, constraints)`, which returns a `Problem` object.\n",
    "\n",
    "The `Problem` object has a function solve(), which returns the minimum of the solution.  In this case, this is the minimum variance of the portfolio.\n",
    "\n",
    "It also updates the vector $\\mathbf{x}$.\n",
    "\n",
    "We can check out the values of $x_A$ and $x_B$ that gave the minimum portfolio variance by using `x.value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "\n",
    "def get_optimal_weights(covariance_returns, index_weights, scale=2.0):\n",
    "    \"\"\"\n",
    "    Find the optimal weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    covariance_returns : 2 dimensional Ndarray\n",
    "        The covariance of the returns\n",
    "    index_weights : Pandas Series\n",
    "        Index weights for all tickers at a period in time\n",
    "    scale : int\n",
    "        The penalty factor for weights the deviate from the index \n",
    "    Returns\n",
    "    -------\n",
    "    x : 1 dimensional Ndarray\n",
    "        The solution for x\n",
    "    \"\"\"\n",
    "    assert len(covariance_returns.shape) == 2\n",
    "    assert len(index_weights.shape) == 1\n",
    "    assert covariance_returns.shape[0] == covariance_returns.shape[1]  == index_weights.shape[0]\n",
    "\n",
    "    #TODO: Implement function\n",
    "    #cf.https://www.cvxpy.org/tutorial/intro/index.html\n",
    "    x = cvx.Variable(len(index_weights))\n",
    "    distance = scale * cvx.norm(x-index_weights, p=2, axis=None)\n",
    "    std = cvx.quad_form(x, covariance_returns)\n",
    "    problem = cvx.Problem(cvx.Minimize(std + distance),\n",
    "                          [x >= 0,sum(x) == 1])\n",
    "    problem.solve()\n",
    "\n",
    "    return x.value\n",
    "\n",
    "project_tests.test_get_optimal_weights(get_optimal_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Portfolio\n",
    "Using the `get_optimal_weights` function, let's generate the optimal ETF weights without rebalanceing. We can do this by feeding in the covariance of the entire history of data. We also need to feed in a set of index weights. We'll go with the average weights of the index over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_optimal_single_rebalance_etf_weights = get_optimal_weights(covariance_returns.values, index_weights.iloc[-1])\n",
    "optimal_single_rebalance_etf_weights = pd.DataFrame(\n",
    "    np.tile(raw_optimal_single_rebalance_etf_weights, (len(returns.index), 1)),\n",
    "    returns.index,\n",
    "    returns.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our ETF weights built, let's compare it to the index. Run the next cell to calculate the ETF returns and compare it to the index returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_etf_returns = generate_weighted_returns(returns, optimal_single_rebalance_etf_weights)\n",
    "optim_etf_cumulative_returns = calculate_cumulative_returns(optim_etf_returns)\n",
    "project_helper.plot_benchmark_returns(index_weighted_cumulative_returns, optim_etf_cumulative_returns, 'Optimized ETF vs Index')\n",
    "\n",
    "optim_etf_tracking_error = tracking_error(np.sum(index_weighted_returns, 1), np.sum(optim_etf_returns, 1))\n",
    "print('Optimized ETF Tracking Error: {}'.format(optim_etf_tracking_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebalance Portfolio Over Time\n",
    "The single optimized ETF portfolio used the same weights for the entire history. This might not be the optimal weights for the entire period. Let's rebalance the portfolio over the same period instead of using the same weights. Implement `rebalance_portfolio` to rebalance a portfolio.\n",
    "\n",
    "Reblance the portfolio every n number of days, which is given as `shift_size`. When rebalancing, you should look back a certain number of days of data in the past, denoted as `chunk_size`. Using this data, compute the optoimal weights using `get_optimal_weights` and `get_covariance_returns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance_portfolio(returns, index_weights, shift_size, chunk_size):\n",
    "    \"\"\"\n",
    "    Get weights for each rebalancing of the portfolio.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    index_weights : DataFrame\n",
    "        Index weight for each ticker and date\n",
    "    shift_size : int\n",
    "        The number of days between each rebalance\n",
    "    chunk_size : int\n",
    "        The number of days to look in the past for rebalancing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_rebalance_weights  : list of Ndarrays\n",
    "        The ETF weights for each point they are rebalanced\n",
    "    \"\"\"\n",
    "    assert returns.index.equals(index_weights.index)\n",
    "    assert returns.columns.equals(index_weights.columns)\n",
    "    assert shift_size > 0\n",
    "    assert chunk_size >= 0\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    all_rebalance_weights = []\n",
    "    for i in range (chunk_size, len(returns), shift_size):\n",
    "        chunked_cov_returns = get_covariance_returns(returns[i-chunk_size:i])\n",
    "        # print(chunked_cov_returns)\n",
    "        # Failed: iWeights = index_weights.iloc[(i-chunk_size):i].mean()\n",
    "        chunked_weights = index_weights.iloc[i-1]\n",
    "        #print(iWeights)\n",
    "        optimal_weights_in_chunk = get_optimal_weights(chunked_cov_returns,\n",
    "                                                       chunked_weights)\n",
    "        #print(optimal_weights_in_chunk)\n",
    "        all_rebalance_weights.append(optimal_weights_in_chunk)\n",
    "    \n",
    "    return all_rebalance_weights\n",
    "\n",
    "project_tests.test_rebalance_portfolio(rebalance_portfolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to rebalance the portfolio using `rebalance_portfolio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 250\n",
    "shift_size = 5\n",
    "all_rebalance_weights = rebalance_portfolio(returns, index_weights, shift_size, chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Turnover\n",
    "With the portfolio rebalanced, we need to use a metric to measure the cost of rebalancing the portfolio. Implement `get_portfolio_turnover` to calculate the annual portfolio turnover. We'll be using the formulas used in the classroom:\n",
    "\n",
    "$ AnnualizedTurnover =\\frac{SumTotalTurnover}{NumberOfRebalanceEvents} * NumberofRebalanceEventsPerYear $\n",
    "\n",
    "$ SumTotalTurnover =\\sum_{t,n}{\\left | x_{t,n} - x_{t+1,n} \\right |} $ Where $ x_{t,n} $ are the weights at time $ t $ for equity $ n $.\n",
    "\n",
    "$ SumTotalTurnover $ is just a different way of writing $ \\sum \\left | x_{t_1,n} - x_{t_2,n} \\right | $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_portfolio_turnover(all_rebalance_weights, shift_size, rebalance_count, n_trading_days_in_year=252):\n",
    "    \"\"\"\n",
    "    Calculage portfolio turnover.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_rebalance_weights : list of Ndarrays\n",
    "        The ETF weights for each point they are rebalanced\n",
    "    shift_size : int\n",
    "        The number of days between each rebalance\n",
    "    rebalance_count : int\n",
    "        Number of times the portfolio was rebalanced\n",
    "    n_trading_days_in_year: int\n",
    "        Number of trading days in a year\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    portfolio_turnover  : float\n",
    "        The portfolio turnover\n",
    "    \"\"\"\n",
    "    assert shift_size > 0\n",
    "    assert rebalance_count > 0\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    #print(all_rebalance_weights)\n",
    "    #print(np.flip(all_rebalance_weights, axis=0))\n",
    "    #print(np.diff(np.flip(all_rebalance_weights, axis=0),axis=0))\n",
    "    sum_total_turnover = np.sum(\n",
    "                            np.abs(\n",
    "                                np.diff(\n",
    "                                    np.flip(\n",
    "                                        all_rebalance_weights,\n",
    "                                        axis=0),\n",
    "                                    axis = 0)\n",
    "                            )\n",
    "                         )\n",
    "    num_of_rebalance_events = n_trading_days_in_year // shift_size\n",
    "    portfolio_turnover = (sum_total_turnover * num_of_rebalance_events)/rebalance_count\n",
    "    \n",
    "    return portfolio_turnover\n",
    "\n",
    "project_tests.test_get_portfolio_turnover(get_portfolio_turnover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to get the portfolio turnover from  `get_portfolio turnover`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_portfolio_turnover(all_rebalance_weights, shift_size, len(all_rebalance_weights) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You've built a smart beta portfolio in part 1 and did portfolio optimization in part 2. You can now submit your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade. You can continue to the next section while you wait for feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('zipline': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b73ca1fdcad561020383f0b3eaf0f57e48567dd5dc903c282620dd473fa7f23e"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}